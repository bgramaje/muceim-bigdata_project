{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b2963f-512b-499c-94e5-ec7f199b2ebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: pyspainmobility in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: geopandas~=1.0.1 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from pyspainmobility) (1.0.1)\n",
      "Requirement already satisfied: tqdm>=4.0.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from pyspainmobility) (4.67.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from pyspainmobility) (3.10.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from pyspainmobility) (22.0.0)\n",
      "Requirement already satisfied: dask[dataframe]>=2024.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from pyspainmobility) (2025.11.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from dask[dataframe]>=2024.0->pyspainmobility) (8.3.1)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from dask[dataframe]>=2024.0->pyspainmobility) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from dask[dataframe]>=2024.0->pyspainmobility) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from dask[dataframe]>=2024.0->pyspainmobility) (25.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from dask[dataframe]>=2024.0->pyspainmobility) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from dask[dataframe]>=2024.0->pyspainmobility) (6.0.3)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from dask[dataframe]>=2024.0->pyspainmobility) (1.1.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from dask[dataframe]>=2024.0->pyspainmobility) (8.7.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from geopandas~=1.0.1->pyspainmobility) (0.11.1)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from geopandas~=1.0.1->pyspainmobility) (3.7.2)\n",
      "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from geopandas~=1.0.1->pyspainmobility) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from matplotlib>=3.0.0->pyspainmobility) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from matplotlib>=3.0.0->pyspainmobility) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from matplotlib>=3.0.0->pyspainmobility) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from matplotlib>=3.0.0->pyspainmobility) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from matplotlib>=3.0.0->pyspainmobility) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from matplotlib>=3.0.0->pyspainmobility) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from tqdm>=4.0.0->pyspainmobility) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from importlib_metadata>=4.13.0->dask[dataframe]>=2024.0->pyspainmobility) (3.23.0)\n",
      "Requirement already satisfied: locket in c:\\users\\joan\\onedrive\\documentos\\estudios\\muceim\\big data engineering and technologies\\project\\.venv-portatil\\lib\\site-packages (from partd>=1.4.0->dask[dataframe]>=2024.0->pyspainmobility) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install duckdb pandas numpy pyspainmobility requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469146e7-75a5-4a84-95d5-8a94ba824658",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><b>Building a 3-Tier Data Lakehouse for Mobility Analysis in Spain</b></h1>\n",
    "<h3 align=\"center\"><b style=\"color:gray\">Bronze Layer</b></h3>\n",
    "<h4 align=\"right\">Joan Fernández Navarro & Borja Albert Gramaje</h4>\n",
    "<h3><b>Table of Contents</b></h3>\n",
    "<ul style = \"list-style-type: none; line-height: 0.5em;\">\n",
    "    <li><a href=\"#mitma\"><h5>1. Spanish Ministry of Transport, Mobility and Urban Agenda\n",
    "(MITMA) Open Data</h5></a></li>\n",
    "    <ul style = \"list-style-type: none; line-height: 1em;\">\n",
    "        <li><a href=\"#od\"><h5>1.1. Origin-destination (OD) trip matrices</h5></a></li>\n",
    "        <li><a href=\"#people\"><h5>1.2. People by day</h5></a></li>\n",
    "        <li><a href=\"#overnight\"><h5>1.3. Overnight stays</h5></a></li>\n",
    "        <li><a href=\"#zones\"><h5>1.4. Zones</h5></a></li>\n",
    "    </ul>\n",
    "    <li><a href=\"#ine\"><h5>2. Spanish National Statistics Institute (INE)</h5></a></li>\n",
    "    <ul style = \"list-style-type: none; line-height: 1em;\">\n",
    "        <li><a href=\"#population\"><h5>2.1. Population by municipio (Padrón)</h5></a></li>\n",
    "        <li><a href=\"#income\"><h5>2.2. Income by distrito</h5></a></li>\n",
    "        <li><a href=\"#business\"><h5>2.3. Business by municipio</h5></a></li>\n",
    "    </ul>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930f01db-a305-4346-8b9b-5e4d0f475d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB version: v1.4.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspainmobility import Mobility, Zones\n",
    "\n",
    "BASE_PATH = f\"{os.getcwd()}/../../raw\"\n",
    "LAKE_LAYER = \"bronze\"\n",
    "\n",
    "con = duckdb.connect(\"./../../mobility.db\")\n",
    "\n",
    "def SQL(q):\n",
    "    \"\"\"Run SQL (printed for clarity) and return a DataFrame.\"\"\"\n",
    "    return con.execute(q).fetchdf()\n",
    "\n",
    "print(\"DuckDB version:\", con.sql(\"SELECT version();\").fetchone()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef23656-53a4-47f1-a8e6-b3ec0d5adc45",
   "metadata": {},
   "source": [
    "<h2 id=\"mitma\"><b>1. Spanish Ministry of Transport, Mobility and Urban Agenda (MITMA) Open Data</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d31218-fc3e-455f-a82a-4cb4fedf5586",
   "metadata": {},
   "source": [
    "<h2 id=\"od\"><b>1.1. Origin-destination (OD) trip matrices</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2058a6-8e65-4b3a-9bf3-dbbde4e61671",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Trip Matrices - distristos\n",
    "CREATE TABLE bronze_mitma_od_districts (\n",
    "    fecha TEXT,\n",
    "    periodo TEXT,\n",
    "    origen TEXT,\n",
    "    destino TEXT,\n",
    "    distancia TEXT,\n",
    "    actividad_origen TEXT,\n",
    "    actividad_destino TEXT,\n",
    "    residencia TEXT,\n",
    "    renta TEXT,\n",
    "    edad TEXT,\n",
    "    sexo TEXT,\n",
    "    viajes TEXT,\n",
    "    viajes_km TEXT,\n",
    "    -- Columnas extras añadidas para auditoria. \n",
    "    loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    source_file TEXT\n",
    ");\n",
    "\n",
    "-- Trip Matrices - municipalities\n",
    "CREATE TABLE bronze_mitma_od_municipalities (\n",
    "    fecha TEXT,\n",
    "    periodo TEXT,\n",
    "    origen TEXT,\n",
    "    destino TEXT,\n",
    "    distancia TEXT,\n",
    "    actividad_origen TEXT,\n",
    "    actividad_destino TEXT,\n",
    "    residencia TEXT,\n",
    "    renta TEXT,\n",
    "    edad TEXT,\n",
    "    sexo TEXT,\n",
    "    viajes TEXT,\n",
    "    viajes_km TEXT,\n",
    "    -- Columnas extras añadidas para auditoria. \n",
    "    loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    source_file TEXT\n",
    ");\n",
    "\n",
    "-- Trip Matrices - GAU\n",
    "CREATE TABLE bronze_mitma_od_gau (\n",
    "    fecha TEXT,\n",
    "    periodo TEXT,\n",
    "    origen TEXT,\n",
    "    destino TEXT,\n",
    "    distancia TEXT,\n",
    "    actividad_origen TEXT,\n",
    "    actividad_destino TEXT,\n",
    "    residencia TEXT,\n",
    "    renta TEXT,\n",
    "    edad TEXT,\n",
    "    sexo TEXT,\n",
    "    viajes TEXT,\n",
    "    viajes_km TEXT,\n",
    "    -- Columnas extras añadidas para auditoria. \n",
    "    loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    source_file TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "![Descripción de la imagen](./schemas/bronze_od.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409aa2be-2143-418e-973b-9da0d262d25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MITMA OD matrices for zone type: distritos...\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230301_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230301_Viajes_distritos.csv.gz\n",
      "Saved 183411650 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230301_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230302_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230302_Viajes_distritos.csv.gz\n",
      "Saved 185935541 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230302_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230303_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230303_Viajes_distritos.csv.gz\n",
      "Saved 193391911 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230303_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230304_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230304_Viajes_distritos.csv.gz\n",
      "Saved 170450687 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230304_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230305_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230305_Viajes_distritos.csv.gz\n",
      "Saved 144499798 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230305_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230306_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230306_Viajes_distritos.csv.gz\n",
      "Saved 178366965 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230306_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230307_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230307_Viajes_distritos.csv.gz\n",
      "Saved 183482043 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230307_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230308_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230308_Viajes_distritos.csv.gz\n",
      "Saved 183813108 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230308_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230309_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230309_Viajes_distritos.csv.gz\n",
      "Saved 186112433 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230309_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230310_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230310_Viajes_distritos.csv.gz\n",
      "Saved 195910951 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230310_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230311_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230311_Viajes_distritos.csv.gz\n",
      "Saved 173021543 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230311_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230312_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230312_Viajes_distritos.csv.gz\n",
      "Saved 152640808 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230312_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230313_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230313_Viajes_distritos.csv.gz\n",
      "Saved 181295677 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230313_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230314_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230314_Viajes_distritos.csv.gz\n",
      "Saved 185559075 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230314_Viajes_distritos_v2.csv.gz\n",
      "Downloading file from https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230315_Viajes_distritos.csv.gz\n",
      "Downloading: https://movilidad-opendata.mitma.es/estudios_basicos/por-distritos/viajes/ficheros-diarios/2023-03/20230315_Viajes_distritos.csv.gz\n",
      "Saved 188331298 bytes to C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230315_Viajes_distritos_v2.csv.gz\n",
      "Generating parquet file for ODs....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230301_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                             | 1/15 [01:34<22:07, 94.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230302_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████                                                                        | 2/15 [03:08<20:24, 94.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230303_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 3/15 [04:43<18:54, 94.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230304_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▏                                                            | 4/15 [05:56<15:45, 85.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230305_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▋                                                       | 5/15 [06:51<12:27, 74.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230306_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 6/15 [08:06<11:15, 75.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230307_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████████▋                                            | 7/15 [09:22<10:01, 75.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230308_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████████████████████████████▎                                      | 8/15 [10:39<08:51, 75.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230309_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 9/15 [12:00<07:43, 77.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230310_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▋                           | 10/15 [13:20<06:31, 78.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230311_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████▏                     | 11/15 [14:27<04:58, 74.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230312_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 12/15 [15:26<03:29, 69.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230313_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████           | 13/15 [16:43<02:24, 72.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230314_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████▌     | 14/15 [17:57<01:12, 72.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\Joan\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\project\\notebooks\\layers/../../raw/MITMA/od_distritos\\20230315_Viajes_distritos_v2.csv.gz\n",
      "Reading gzipped file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [19:14<00:00, 76.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating all the dataframes....\n",
      "Writing the parquet file....\n"
     ]
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 1073741824 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     65\u001b[39m     SQL(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[33m        INSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33m        SELECT\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m \u001b[33m        );\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData successfully loaded into table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mload_od_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzone_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdistritos\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2023-03-01\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2023-03-15\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m load_od_matrices(zone_type=\u001b[33m\"\u001b[39m\u001b[33mmunicipios\u001b[39m\u001b[33m\"\u001b[39m, start_date=\u001b[33m\"\u001b[39m\u001b[33m2023-03-01\u001b[39m\u001b[33m\"\u001b[39m, end_date=\u001b[33m\"\u001b[39m\u001b[33m2023-03-15\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m load_od_matrices(zone_type=\u001b[33m\"\u001b[39m\u001b[33mgau\u001b[39m\u001b[33m\"\u001b[39m, start_date=\u001b[33m\"\u001b[39m\u001b[33m2023-03-01\u001b[39m\u001b[33m\"\u001b[39m, end_date=\u001b[33m\"\u001b[39m\u001b[33m2023-03-15\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mload_od_matrices\u001b[39m\u001b[34m(zone_type, start_date, end_date)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading MITMA OD matrices for zone type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzone_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m mobility = Mobility(\n\u001b[32m     29\u001b[39m     version=\u001b[32m2\u001b[39m,\n\u001b[32m     30\u001b[39m     zones=zone_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     output_directory=\u001b[38;5;28mstr\u001b[39m(dataset_path),\n\u001b[32m     34\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mmobility\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_od_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep_activity\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 3. Create the target DuckDB table\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------\u001b[39;00m\n\u001b[32m     40\u001b[39m SQL(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[33m    CREATE TABLE IF NOT EXISTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[33m        fecha TEXT,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33m    );\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyspainmobility\\mobility\\mobility.py:288\u001b[39m, in \u001b[36mMobility.get_od_data\u001b[39m\u001b[34m(self, keep_activity, return_df, social_agg)\u001b[39m\n\u001b[32m    285\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConcatenating all the dataframes....\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    286\u001b[39m         df = temp_dfs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(temp_dfs) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m pd.concat(temp_dfs)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_saving_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df \u001b[38;5;28;01mif\u001b[39;00m return_df \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.version == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyspainmobility\\mobility\\mobility.py:625\u001b[39m, in \u001b[36mMobility._saving_parquet\u001b[39m\u001b[34m(self, df, m_type)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_saving_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd.DataFrame, m_type: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mWriting the parquet file....\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mm_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzones\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_v\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mParquet file generated successfully at \u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    630\u001b[39m           os.path.join(\u001b[38;5;28mself\u001b[39m.output_path, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.zones\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.start_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.end_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pandas\\io\\parquet.py:482\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m impl = get_engine(engine)\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pandas\\io\\parquet.py:191\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     from_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpreserve_index\u001b[39m\u001b[33m\"\u001b[39m] = index\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfrom_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.attrs:\n\u001b[32m    194\u001b[39m     df_metadata = {\u001b[33m\"\u001b[39m\u001b[33mPANDAS_ATTRS\u001b[39m\u001b[33m\"\u001b[39m: json.dumps(df.attrs)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyarrow\\table.pxi:4795\u001b[39m, in \u001b[36mpyarrow.lib.Table.from_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyarrow\\pandas_compat.py:653\u001b[39m, in \u001b[36mdataframe_to_arrays\u001b[39m\u001b[34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[39m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, maybe_fut \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[32m    652\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_fut, futures.Future):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m             arrays[i] = \u001b[43mmaybe_fut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m types = [x.type \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyarrow\\pandas_compat.py:622\u001b[39m, in \u001b[36mdataframe_to_arrays.<locals>.convert_column\u001b[39m\u001b[34m(col, field)\u001b[39m\n\u001b[32m    619\u001b[39m     type_ = field.type\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     result = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (pa.ArrowInvalid,\n\u001b[32m    624\u001b[39m         pa.ArrowNotImplementedError,\n\u001b[32m    625\u001b[39m         pa.ArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    626\u001b[39m     e.args += (\n\u001b[32m    627\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion failed for column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyarrow\\array.pxi:312\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyarrow\\array.pxi:118\u001b[39m, in \u001b[36mpyarrow.lib._handle_arrow_array_protocol\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pandas\\core\\arrays\\string_.py:757\u001b[39m, in \u001b[36mStringArray.__arrow_array__\u001b[39m\u001b[34m(self, type)\u001b[39m\n\u001b[32m    755\u001b[39m values = \u001b[38;5;28mself\u001b[39m._ndarray.copy()\n\u001b[32m    756\u001b[39m values[\u001b[38;5;28mself\u001b[39m.isna()] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyarrow\\array.pxi:365\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyarrow\\array.pxi:91\u001b[39m, in \u001b[36mpyarrow.lib._ndarray_to_array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\Estudios\\MUCEIM\\Big Data Engineering and Technologies\\Project\\.venv-portatil\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowMemoryError\u001b[39m: realloc of size 1073741824 failed"
     ]
    }
   ],
   "source": [
    "def load_od_matrices(zone_type=\"districts\", start_date=\"2022-03-01\", end_date=\"2022-03-03\"):\n",
    "    \"\"\"\n",
    "    Downloads MITMA OD matrices (only if not already present) and loads them into DuckDB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zone_type : str\n",
    "        Zone level (\"districts\", \"municipalities\", etc.).\n",
    "    start_date : str\n",
    "        Start date (YYYY-MM-DD).\n",
    "    end_date : str\n",
    "        End date (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = \"od\"\n",
    "    dataset_path = f\"{BASE_PATH}/MITMA/{dataset}_{zone_type}\"\n",
    "    table_name = f\"{LAKE_LAYER}_mitma_{dataset}_{zone_type}\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1. Ensure directory exists\n",
    "    # -------------------------------------------------------\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Download data only if the directory is empty\n",
    "    # -------------------------------------------------------\n",
    "    print(f\"Downloading MITMA OD matrices for zone type: {zone_type}...\")\n",
    "    mobility = Mobility(\n",
    "        version=2,\n",
    "        zones=zone_type,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        output_directory=str(dataset_path),\n",
    "    )\n",
    "    mobility.get_od_data(keep_activity=True)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Create the target DuckDB table\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name}(\n",
    "            fecha TEXT,\n",
    "            periodo TEXT,\n",
    "            origen TEXT,\n",
    "            destino TEXT,\n",
    "            distancia TEXT,\n",
    "            actividad_origen TEXT,\n",
    "            actividad_destino TEXT,\n",
    "            residencia TEXT,\n",
    "            renta TEXT,\n",
    "            edad TEXT,\n",
    "            sexo TEXT,\n",
    "            viajes TEXT,\n",
    "            viajes_km TEXT,\n",
    "            estudio_destino_posible TEXT,\n",
    "            estudio_origen_posible TEXT,\n",
    "            loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            source_file TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4. Load the CSV files into the table\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        SELECT\n",
    "            fecha,\n",
    "            periodo,\n",
    "            origen,\n",
    "            destino,\n",
    "            distancia,\n",
    "            actividad_origen,\n",
    "            actividad_destino,\n",
    "            residencia,\n",
    "            renta,\n",
    "            edad,\n",
    "            sexo,\n",
    "            viajes,\n",
    "            viajes_km,\n",
    "            estudio_destino_posible,\n",
    "            estudio_origen_posible,\n",
    "            CURRENT_TIMESTAMP AS loaded_at,\n",
    "            filename AS source_file\n",
    "        FROM read_csv(\n",
    "            '{dataset_path}/*.csv.gz',\n",
    "            filename = true,\n",
    "            all_varchar = true\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"Data successfully loaded into table: {table_name}\")\n",
    "\n",
    "load_od_matrices(zone_type=\"distritos\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")\n",
    "load_od_matrices(zone_type=\"municipios\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")\n",
    "load_od_matrices(zone_type=\"gau\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855f89b-e73b-49d9-a6eb-8f01f6eff722",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "    (SELECT '{LAKE_LAYER}_mitma_od_distritos' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_od_distritos)\n",
    "        UNION\n",
    "    (SELECT '{LAKE_LAYER}_mitma_od_municipios' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_od_municipios)\n",
    "        UNION\n",
    "    (SELECT '{LAKE_LAYER}_mitma_od_gau' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_od_gau);\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25428cf3-39e4-4b82-83bb-f8c8e07443a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {LAKE_LAYER}_mitma_od_distritos \n",
    "    LIMIT 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5841c-2b20-4466-8d53-fb436f98f646",
   "metadata": {},
   "source": [
    "<h2 id=\"people\"><b>1.2. People by day</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33138b21-9b1b-446e-9661-471adfdbb4ff",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Distritos\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_people_day_districts (\n",
    "  fecha TEXT,\n",
    "  zona_pernoctacion TEXT,\n",
    "  edad TEXT,\n",
    "  sexo TEXT,\n",
    "  numero_viajes TEXT,   -- 0,1,2,2+ (mantener TEXT)\n",
    "  personas TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "\n",
    "-- Municipios\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_people_day_municipalities (\n",
    "  fecha TEXT,\n",
    "  zona_pernoctacion TEXT,\n",
    "  edad TEXT,\n",
    "  sexo TEXT,\n",
    "  numero_viajes TEXT,\n",
    "  personas TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "\n",
    "-- GAU\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_people_day_gau (\n",
    "  fecha TEXT,\n",
    "  zona_pernoctacion TEXT,\n",
    "  edad TEXT,\n",
    "  sexo TEXT,\n",
    "  numero_viajes TEXT,\n",
    "  personas TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "![Descripción de la imagen](./schemas/bronze_people_day.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a85823-65b8-47f0-92bf-db20a0bd64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_people_day(zone_type=\"districts\", start_date=\"2022-03-01\", end_date=\"2022-03-03\"):\n",
    "    \"\"\"\n",
    "    Downloads MITMA 'people_day' data (only if not already present)\n",
    "    and loads it into DuckDB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zone_type : str\n",
    "        Zone level (“districts”, “municipalities”, etc.).\n",
    "    start_date : str\n",
    "        Start date (YYYY-MM-DD).\n",
    "    end_date : str\n",
    "        End date (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = \"people_day\"\n",
    "    dataset_path = f\"{BASE_PATH}/MITMA/{dataset}_{zone_type}\"\n",
    "    table_name = f\"{LAKE_LAYER}_mitma_{dataset}_{zone_type}\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1. Ensure directory exists\n",
    "    # -------------------------------------------------------\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Download data only if directory is empty\n",
    "    # -------------------------------------------------------\n",
    "    print(f\"Downloading MITMA 'people_day' dataset for: {zone_type}...\")\n",
    "    mobility = Mobility(\n",
    "        version=2,\n",
    "        zones=zone_type,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        output_directory=str(dataset_path),\n",
    "    )\n",
    "    mobility.get_number_of_trips_data()\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Create DuckDB table\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        CREATE TABLE {table_name}(\n",
    "            fecha TEXT,\n",
    "            zona_pernoctacion TEXT,\n",
    "            edad TEXT,\n",
    "            sexo TEXT,\n",
    "            numero_viajes TEXT,\n",
    "            personas TEXT,\n",
    "            loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            source_file TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4. Load data from CSV into DuckDB\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        SELECT\n",
    "            fecha,\n",
    "            zona_pernoctacion,\n",
    "            edad,\n",
    "            sexo,\n",
    "            numero_viajes,\n",
    "            personas,\n",
    "            CURRENT_TIMESTAMP AS loaded_at,\n",
    "            filename AS source_file\n",
    "        FROM read_csv(\n",
    "            '{dataset_path}/*.csv.gz',\n",
    "            filename = true,\n",
    "            all_varchar = true\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"Data successfully loaded into table: {table_name}\")\n",
    "\n",
    "\n",
    "load_people_day(zone_type=\"distritos\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")\n",
    "load_people_day(zone_type=\"municipios\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")\n",
    "load_people_day(zone_type=\"gau\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0726a7f-dd84-4120-8e79-3d5c1aad131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "    (SELECT '{LAKE_LAYER}_mitma_people_day_distritos' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_people_day_distritos)\n",
    "        UNION\n",
    "    (SELECT '{LAKE_LAYER}_mitma_people_day_municipios' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_people_day_municipios)\n",
    "        UNION\n",
    "    (SELECT '{LAKE_LAYER}_mitma_people_day_gau' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_people_day_gau);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d69068-62e8-4c55-a16f-4a80a7bee100",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {LAKE_LAYER}_mitma_people_day_distritos \n",
    "    LIMIT 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70d94c-2fae-4ab1-a960-4088707f3137",
   "metadata": {},
   "source": [
    "<h2 id=\"overnight\"><b>1.3. Overnight stays</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de363a18-ffe9-4cec-8f17-0d0ff9a7fcdf",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Distritos\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_overnight_stay_districts (\n",
    "  fecha TEXT,\n",
    "  zona_residencia TEXT,\n",
    "  zona_pernoctacion TEXT,\n",
    "  personas TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "\n",
    "-- Municipios\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_overnight_stay_municipalities (\n",
    "  fecha TEXT,\n",
    "  zona_residencia TEXT,\n",
    "  zona_pernoctacion TEXT,\n",
    "  personas TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "\n",
    "-- GAU\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_overnight_stay_gau (\n",
    "  fecha TEXT,\n",
    "  zona_residencia TEXT,\n",
    "  zona_pernoctacion TEXT,\n",
    "  personas TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "![Descripción de la imagen](./schemas/bronze_overnight.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c9735-5c8f-4e85-9332-b40c6aae3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_overnight_stay(zone_type=\"districts\", start_date=\"2022-03-01\", end_date=\"2022-03-03\"):\n",
    "    \"\"\"\n",
    "    Downloads MITMA overnight-stay data (only if missing) and loads it into DuckDB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zone_type : str\n",
    "        Zone level (“districts”, “municipalities”, etc.).\n",
    "    start_date : str\n",
    "        Start date (YYYY-MM-DD).\n",
    "    end_date : str\n",
    "        End date (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = \"overnight_stay\"\n",
    "    dataset_path = f\"{BASE_PATH}/MITMA/{dataset}_{zone_type}\"\n",
    "    table_name = f\"{LAKE_LAYER}_mitma_{dataset}_{zone_type}\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1. Ensure directory exists\n",
    "    # -------------------------------------------------------\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Download data only if directory is empty\n",
    "    # -------------------------------------------------------\n",
    "    print(f\"Downloading MITMA overnight-stay data for zone type: {zone_type}...\")\n",
    "    mobility = Mobility(\n",
    "        version=2,\n",
    "        zones=zone_type,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        output_directory=str(dataset_path),\n",
    "    )\n",
    "    mobility.get_overnight_stays_data()\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Create table in DuckDB\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        CREATE TABLE {table_name}(\n",
    "            fecha TEXT,\n",
    "            zona_residencia TEXT,\n",
    "            zona_pernoctacion TEXT,\n",
    "            personas TEXT,\n",
    "            loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            source_file TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4. Insert CSV data\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        SELECT\n",
    "            fecha,\n",
    "            zona_residencia,\n",
    "            zona_pernoctacion,\n",
    "            personas,\n",
    "            CURRENT_TIMESTAMP AS loaded_at,\n",
    "            filename AS source_file\n",
    "        FROM read_csv(\n",
    "            '{dataset_path}/*.csv.gz',\n",
    "            filename = true,\n",
    "            all_varchar = true\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"Overnight-stay data successfully loaded into table: {table_name}\")\n",
    "\n",
    "\n",
    "load_overnight_stay(zone_type=\"distritos\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")\n",
    "load_overnight_stay(zone_type=\"municipios\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")\n",
    "load_overnight_stay(zone_type=\"gau\", start_date=\"2023-03-01\", end_date=\"2023-03-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5937a-d5e2-4cd1-919e-14f8927e0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "    (SELECT '{LAKE_LAYER}_mitma_overnight_stay_distritos' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_overnight_stay_distritos)\n",
    "        UNION\n",
    "    (SELECT '{LAKE_LAYER}_mitma_overnight_stay_municipios' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_overnight_stay_municipios)\n",
    "        UNION\n",
    "    (SELECT '{LAKE_LAYER}_mitma_overnight_stay_gau' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_overnight_stay_gau);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348b624-ed7e-43b7-bc67-26a19301ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {LAKE_LAYER}_mitma_overnight_stay_distritos \n",
    "    LIMIT 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682300fa-a5a5-4955-a820-619b5b5077c3",
   "metadata": {},
   "source": [
    "<h2 id=\"zones\"><b>1.4. Zones</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe6059-1c16-4b36-9357-0cb7ca143087",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Distritos\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_districts (\n",
    "  id TEXT,\n",
    "  name TEXT,\n",
    "  population TEXT,\n",
    "  geometry TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "\n",
    "-- Municipios\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_municipalities (\n",
    "  id TEXT,\n",
    "  name TEXT,\n",
    "  population TEXT,\n",
    "  geometry TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "\n",
    "-- GAU\n",
    "CREATE TABLE IF NOT EXISTS bronze_mitma_gau (\n",
    "  id TEXT,\n",
    "  name TEXT,\n",
    "  population TEXT,\n",
    "  geometry TEXT,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "\n",
    "```\n",
    "\n",
    "![Descripción de la imagen](./schemas/bronze_zones.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a217dd4-31a0-4586-a61f-2bb0f6fb3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zones(zone_type=\"districts\"):\n",
    "    \"\"\"\n",
    "    Downloads MITMA zone definitions (only if missing), stores them as compressed CSV,\n",
    "    and loads them into DuckDB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zone_type : str\n",
    "        Zone level (“districts”, “municipalities”, “gau”, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = \"zones\"\n",
    "    dataset_path = f\"{BASE_PATH}/MITMA/{zone_type}\"\n",
    "    table_name = f\"{LAKE_LAYER}_mitma_{zone_type}\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1. Ensure directory exists\n",
    "    # -------------------------------------------------------\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    csv_path = dataset_path + \"/zones.csv.gz\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Download and save zones CSV if not present\n",
    "    # -------------------------------------------------------\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        print(f\"Downloading MITMA zone definitions for zone type: {zone_type}...\")\n",
    "\n",
    "        zones = Zones(\n",
    "            version=2,\n",
    "            zones=zone_type,\n",
    "            output_directory=str(dataset_path),\n",
    "        )\n",
    "\n",
    "        df = zones.get_zone_geodataframe()\n",
    "\n",
    "        if df is None:\n",
    "            raise ValueError(\"Zones.get_zone_geodataframe() returned None\")\n",
    "\n",
    "        # Save geodataframe as compressed CSV\n",
    "        df.to_csv(csv_path, index=True, compression=\"gzip\")\n",
    "        print(\"Zones saved:\", csv_path)\n",
    "    \n",
    "    else:\n",
    "        print(\"Zone definition file already exists. Skipping download.\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Create DuckDB table\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        CREATE TABLE {table_name}(\n",
    "            id TEXT,\n",
    "            name TEXT,\n",
    "            population TEXT,\n",
    "            geometry TEXT,   -- stored as plain text in BRONZE layer\n",
    "            loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            source_file TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4. Load CSV into DuckDB\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        SELECT\n",
    "            id,\n",
    "            name,\n",
    "            population,\n",
    "            geometry,\n",
    "            CURRENT_TIMESTAMP AS loaded_at,\n",
    "            filename AS source_file\n",
    "        FROM read_csv(\n",
    "            '{dataset_path}/*.csv.gz',\n",
    "            filename = true,\n",
    "            all_varchar = true\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"Zone data successfully loaded into table: {table_name}\")\n",
    "\n",
    "\n",
    "\n",
    "load_zones(zone_type=\"distritos\")\n",
    "load_zones(zone_type=\"municipios\")\n",
    "load_zones(zone_type=\"gau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40a76e-7d97-4011-8483-3eac5086db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "    (SELECT '{LAKE_LAYER}_mitma_distritos' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_distritos)\n",
    "        UNION\n",
    "    (SELECT '{LAKE_LAYER}_mitma_municipios' as name, count(*) \n",
    "    FROM {LAKE_LAYER}_mitma_municipios)\n",
    "        UNION\n",
    "    (SELECT '{LAKE_LAYER}_mitma_gau' as name, count(*)\n",
    "    FROM {LAKE_LAYER}_mitma_gau);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb2834-a7b3-4910-992c-c2369a4feb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {LAKE_LAYER}_mitma_od_distritos \n",
    "    LIMIT 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0722f8a-a091-4c69-9718-ca23a15cdaef",
   "metadata": {},
   "source": [
    "<h2 id=\"ine\"><b>2. Spanish National Statistics Institute (INE)</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a883965-c5ae-4871-88ba-f86336ab857d",
   "metadata": {},
   "source": [
    "<h2 id=\"population\"><b>2.1. Population by Municipio (Padrón)</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407b3ace-45a5-4094-b772-1c7f0d8681f7",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Distritos\n",
    "CREATE TABLE IF NOT EXISTS bronze_ine_padron_municipios (\n",
    "  cod        VARCHAR,\n",
    "  nombre     VARCHAR,\n",
    "  fk_unidad  INTEGER,\n",
    "  fk_escala  INTEGER,\n",
    "  data_txt   TEXT,\n",
    "  data       JSON,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  -- loaded_by TEXT DEFAULT CURRENT_USER,\n",
    "  source_file TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "![Descripción de la imagen](./schemas/bronze_ine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c674ca-3208-40ba-b5d3-5236f97af50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padron_by_municipio(year: int):\n",
    "    \"\"\"\n",
    "    Fetches Spanish municipal population (Padrón) data from INE's WS Tempus API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year for which the Padrón data is requested.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Normalized JSON response as DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://servicios.ine.es/wstempus/js/ES/DATOS_TABLA/29005?date={year}0101:{year}1231\"\n",
    "\n",
    "    response = requests.get(url, timeout=120)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    df = pd.json_normalize(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_padron(year=2023):\n",
    "    \"\"\"\n",
    "    Downloads INE municipal Padrón data (only if missing) and loads it into DuckDB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year of the Padrón dataset to retrieve.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = \"padron_municipios\"\n",
    "    dataset_path = f\"{BASE_PATH}/INE/{dataset}\"\n",
    "    table_name = f\"{LAKE_LAYER}_ine_{dataset}\"\n",
    "\n",
    "    filename = f\"padron_municipios_{year}.csv.gz\"\n",
    "    file_path = dataset_path + \"/\" + filename\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1. Ensure directory exists\n",
    "    # -------------------------------------------------------\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Download and save zones CSV if not present\n",
    "    # -------------------------------------------------------\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        print(f\"Downloading INE Padrón municipal data for year {year}...\")\n",
    "\n",
    "        df = get_padron_by_municipio(year)\n",
    "\n",
    "        if df is not None and not df.empty:\n",
    "            df.to_csv(file_path, index=False, compression=\"gzip\")\n",
    "        else:\n",
    "            raise ValueError(\"get_padron_by_municipio() returned an empty or null dataframe\")\n",
    "    else:\n",
    "        print(\"File already exists. Skipping download.\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Create DuckDB table\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        CREATE TABLE {table_name}(\n",
    "            cod        VARCHAR,\n",
    "            nombre     VARCHAR,\n",
    "            fk_unidad  INTEGER,\n",
    "            fk_escala  INTEGER,\n",
    "            data_txt   TEXT,\n",
    "            data       JSON,\n",
    "            loaded_at  TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            source_file TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4. Insert CSV data into DuckDB\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        SELECT\n",
    "            COD        AS cod,\n",
    "            Nombre     AS nombre,\n",
    "            FK_Unidad::INTEGER AS fk_unidad,\n",
    "            FK_Escala::INTEGER AS fk_escala,\n",
    "            REGEXP_REPLACE(\n",
    "                REGEXP_REPLACE(\n",
    "                    REGEXP_REPLACE(\n",
    "                        REPLACE(data, '''', '\"'),\n",
    "                        '\\\\bTrue\\\\b', 'true'\n",
    "                    ),\n",
    "                    '\\\\bFalse\\\\b', 'false'\n",
    "                ),\n",
    "                '\\\\bNone\\\\b', 'null'\n",
    "            ) AS data_txt,\n",
    "            CAST(data_txt AS JSON) AS data,\n",
    "            CURRENT_TIMESTAMP AS loaded_at,\n",
    "            filename AS source_file\n",
    "        FROM read_csv(\n",
    "            '{file_path}',\n",
    "            filename = true,\n",
    "            all_varchar = true\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"INE Padrón data for year {year} successfully loaded into table: {table_name}\")\n",
    "\n",
    "\n",
    "load_padron(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff39b0-e3e8-4e0d-b858-9ccd9721005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "    SELECT * \n",
    "    FROM bronze_ine_padron_municipios \n",
    "    LIMIT 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cceb15-f175-4207-ada5-dbf785a9b46c",
   "metadata": {},
   "source": [
    "<h2 id=\"income\"><b>2.2. Income by District</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189dea31-1767-4205-95c7-bbaaf2756350",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Distritos\n",
    "CREATE TABLE IF NOT EXISTS bronze_ine_renta_distritos (\n",
    "  municipio      VARCHAR,\n",
    "  distrito       VARCHAR,\n",
    "  seccion        VARCHAR,\n",
    "  indicador      VARCHAR,\n",
    "  anyo           INTEGER,\n",
    "  total          VARCHAR,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "![Descripción de la imagen](./schemas/bronze_income.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea599c86-85c5-4de1-8acc-5f5028f2dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_income():\n",
    "    \"\"\"\n",
    "    Loads the income data into DuckDB.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = \"renta_distritos\"\n",
    "    dataset_path = f\"{BASE_PATH}/INE/{dataset}.csv\"\n",
    "    table_name = f\"{LAKE_LAYER}_ine_{dataset}\"\n",
    "\n",
    " # -------------------------------------------------------\n",
    "    # 1. Ensure CSV is downloaded\n",
    "    # -------------------------------------------------------\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        print(\"'renta_distritos' data is not available.\")\n",
    "    else:\n",
    "        print(\"Loading data...\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Create DuckDB table\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            municipio      VARCHAR,\n",
    "            distrito       VARCHAR,\n",
    "            seccion        VARCHAR,\n",
    "            indicador      VARCHAR,\n",
    "            anyo           INTEGER,\n",
    "            total          VARCHAR,\n",
    "            loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            source_file TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Insert CSV data into DuckDB\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        SELECT\n",
    "            Municipios                      AS municipio,\n",
    "            Distritos                       AS distrito,\n",
    "            Secciones                       AS seccion,\n",
    "            Indicador                       AS indicador,\n",
    "            Periodo::INTEGER                AS anyo,\n",
    "            Total                           AS total,\n",
    "            CURRENT_TIMESTAMP AS loaded_at,\n",
    "            filename AS source_file\n",
    "        FROM read_csv(\n",
    "            '{dataset_path}',\n",
    "            filename = true,\n",
    "            all_varchar = true\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"INE INCOME data successfully loaded into table: {table_name}\")\n",
    "\n",
    "\n",
    "load_income()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb91288-a1fb-4f23-8840-f1f6118fb783",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "    SELECT 'bronze_ine_renta_distritos' as name, COUNT(*)\n",
    "    FROM bronze_ine_renta_distritos;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05858379-59b4-44da-90a9-9f0fcc135975",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "    SELECT * \n",
    "    FROM bronze_ine_renta_distritos \n",
    "    LIMIT 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad338e4-ada4-4cc1-a7ab-573ca0de4763",
   "metadata": {},
   "source": [
    "<h2 id=\"business\"><b>2.3. Business by municipio</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9e262-5766-4ee7-991a-ba39df701e12",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Municipios\n",
    "CREATE TABLE IF NOT EXISTS bronze_ine_empresas_municipios (\n",
    "  municipio      VARCHAR,\n",
    "  anyo           INTEGER,\n",
    "  total          VARCHAR,\n",
    "  -- Columnas extras añadidas para auditoria. \n",
    "  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  source_file TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "![Descripción de la imagen](./schemas/bronze_business.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbbcd8-bde7-459b-80d2-d5ec2f8e5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_business():\n",
    "    \"\"\"\n",
    "    Loads the number of business data into DuckDB.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = \"empresas_municipios\"\n",
    "    dataset_path = f\"{BASE_PATH}/INE/{dataset}.csv\"\n",
    "    table_name = f\"{LAKE_LAYER}_ine_{dataset}\"\n",
    "\n",
    " # -------------------------------------------------------\n",
    "    # 1. Ensure CSV is downloaded\n",
    "    # -------------------------------------------------------\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        print(\"'empresas_municipios' data is not available.\")\n",
    "    else:\n",
    "        print(\"Loading data...\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Create DuckDB table\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            municipio      VARCHAR,\n",
    "            anyo           INTEGER,\n",
    "            total          VARCHAR,\n",
    "            loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            source_file TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Insert CSV data into DuckDB\n",
    "    # -------------------------------------------------------\n",
    "    SQL(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        SELECT\n",
    "            Municipios                      AS municipio,\n",
    "            Periodo::INTEGER                AS anyo,\n",
    "            Total                           AS total,\n",
    "            CURRENT_TIMESTAMP AS loaded_at,\n",
    "            filename AS source_file\n",
    "        FROM read_csv(\n",
    "            '{dataset_path}',\n",
    "            delim=';',\n",
    "            header=True,\n",
    "            encoding='latin-1',\n",
    "            filename=true,\n",
    "            all_varchar=true\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"INE INCOME data successfully loaded into table: {table_name}\")\n",
    "\n",
    "\n",
    "load_business()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ee65d-b813-4107-a2bf-d4f6cff15b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "    SELECT 'bronze_ine_empresas_municipios' as name, COUNT(*)\n",
    "    FROM bronze_ine_empresas_municipios;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041afd3-92c9-429e-a440-294ff2c71c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "    SELECT * \n",
    "    FROM bronze_ine_empresas_municipios \n",
    "    LIMIT 5;\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
